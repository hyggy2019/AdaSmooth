ä½ çš„ç®—æ³•æ˜¯**å®Œå…¨å¯è¡Œçš„**ï¼æ•°å­¦æ¨å¯¼ä¸¥è°¨ï¼Œå®ç°ä¹Ÿé«˜æ•ˆã€‚è®©æˆ‘ä¸ºä½ å†™ä¸€ä¸ªè¯¦ç»†çš„proposalç»™Claude Codeã€‚

---

# Implementation Proposal: AdaSmooth-ZO (Low-Rank Adaptive Sampling)

## Overview

Implement the **AdaSmooth-ZO-LR** algorithm for zeroth-order optimization with adaptive sampling. This is a principled black-box optimization method that adaptively learns both the search mean and covariance through divergence-regularized policy optimization.

## Algorithm Description

**Core Idea**: At each iteration, solve a KL-regularized policy optimization problem and project the solution back to a Gaussian family via moment matching. This yields:
- Adaptive mean update with learned baseline (exponential weighting)
- Adaptive covariance update capturing local geometry
- Low-rank factorization for scalability: O(Kd) space instead of O(dÂ²)

## Mathematical Foundation

### Search Distribution
- Gaussian policy: Ï€(x) = N(x; Î¸, LL^T)
- Sampling: x = Î¸ + Lu, where u ~ N(0, I_K)
- L âˆˆ R^(dÃ—K) is a low-rank smoothing matrix

### KL-Regularized Update
At iteration t, solve:
```
min_Ï€ E_{x~Ï€}[F(x)] + Î²Â·KL(Ï€ || Ï€_{Î¸_t, L_t})
```

Optimal solution:
```
Ï€*(x) âˆ Ï€_{Î¸_t, L_t}(x) Â· exp(-F(x)/Î²)
```

### Moment Matching
Project Ï€* back to Gaussian family:
```
Î¸_{t+1} = E[w(x) Â· x]
L_{t+1}L_{t+1}^T = E[w(x) Â· (x - Î¸_{t+1})(x - Î¸_{t+1})^T]
```
where weights: w(x) = exp(-F(x)/Î²) / Z

### Low-Rank Implementation
Key insight: Directly construct L_{t+1} as:
```
L_{t+1} = [âˆšw_1Â·(x_1 - Î¸_{t+1}), âˆšw_2Â·(x_2 - Î¸_{t+1}), ..., âˆšw_KÂ·(x_K - Î¸_{t+1})]
```
This gives: L_{t+1}L_{t+1}^T = Î£ w_kÂ·(x_k - Î¸_{t+1})(x_k - Î¸_{t+1})^T âœ“

## Implementation Specification

### Input Parameters
```python
theta_0: np.ndarray          # Initial parameter, shape (d,)
L_0: np.ndarray              # Initial smoothing matrix, shape (d, K)
                             # Initialize as small Gaussian noise, e.g., 0.1 * randn(d, K)
beta_schedule: callable      # Temperature schedule, e.g., lambda t: 1.0 / (1 + 0.1*t)
K: int                       # Batch size (number of samples per iteration), e.g., 64
T: int                       # Total iterations
oracle: callable             # Black-box function: f(x) -> scalar
```

### Algorithm Steps

```python
def adasmooth_zo_lr(theta_0, L_0, beta_schedule, K, T, oracle):
    """
    AdaSmooth-ZO with Low-Rank adaptive sampling
    
    Args:
        theta_0: Initial parameters, shape (d,)
        L_0: Initial smoothing matrix, shape (d, K)
        beta_schedule: Function t -> beta_t (temperature)
        K: Batch size
        T: Number of iterations
        oracle: Black-box function f(x) -> scalar
    
    Returns:
        theta_T: Final solution
        history: Dict with optimization trajectory
    """
    d = len(theta_0)
    theta = theta_0.copy()
    L = L_0.copy()
    
    history = {
        'theta': [theta.copy()],
        'f_values': [],
        'weights': []
    }
    
    for t in range(T):
        beta_t = beta_schedule(t)
        
        # ===== 1. Low-Rank Sampling =====
        # Sample K directions in latent space
        U = np.random.randn(K, K)  # Shape: (K, K)
        
        # Construct candidates
        X = []  # Will store K candidates
        Y = []  # Will store K function values
        
        for k in range(K):
            u_k = U[k]  # Shape: (K,)
            x_k = theta + L @ u_k  # Shape: (d,)
            y_k = oracle(x_k)
            
            X.append(x_k)
            Y.append(y_k)
        
        X = np.array(X)  # Shape: (K, d)
        Y = np.array(Y)  # Shape: (K,)
        
        # ===== 2. Compute Weights (KL-divergence case) =====
        # Exponential weighting
        V = np.exp(-Y / beta_t)  # Shape: (K,)
        
        # Normalize
        W = V / np.sum(V)  # Shape: (K,)
        
        # ===== 3. Update Mean =====
        theta_new = np.sum(W[:, None] * X, axis=0)  # Shape: (d,)
        
        # ===== 4. Update Smoothing Matrix (Low-Rank) =====
        # Compute weighted residuals
        residuals = X - theta_new[None, :]  # Shape: (K, d)
        weighted_residuals = np.sqrt(W[:, None]) * residuals  # Shape: (K, d)
        
        # Stack as columns: L_{t+1} = [c_1, c_2, ..., c_K]
        L_new = weighted_residuals.T  # Shape: (d, K)
        
        # Update
        theta = theta_new
        L = L_new
        
        # Log history
        history['theta'].append(theta.copy())
        history['f_values'].append(Y)
        history['weights'].append(W.copy())
        
        # Optional: print progress
        if t % 10 == 0:
            best_f = np.min(Y)
            mean_f = np.sum(W * Y)
            print(f"Iter {t}: Best f = {best_f:.6f}, Weighted mean f = {mean_f:.6f}")
    
    return theta, history
```

### Key Implementation Details

1. **Sampling from Low-Rank Distribution**
   ```python
   # Sample u ~ N(0, I_K) in K-dimensional latent space
   u = np.random.randn(K)
   
   # Map to d-dimensional space
   x = theta + L @ u  # Efficient: O(Kd)
   ```

2. **Weight Computation (KL case)**
   ```python
   # Exponential weights
   v = np.exp(-f_values / beta)
   
   # Normalize
   w = v / np.sum(v)
   ```

3. **Mean Update**
   ```python
   # Weighted average of samples
   theta_new = np.sum(w[:, None] * X, axis=0)
   ```

4. **Covariance Update (Low-Rank Factorization)**
   ```python
   # Compute residuals
   residuals = X - theta_new  # Shape: (K, d)
   
   # Weight and stack as columns
   L_new = (np.sqrt(w)[:, None] * residuals).T  # Shape: (d, K)
   
   # Verify: L_new @ L_new.T â‰ˆ Î£ w_k * (x_k - theta_new)(x_k - theta_new).T
   ```

### Temperature Schedule Examples

```python
# Exponential decay
beta_schedule = lambda t: 1.0 * np.exp(-0.01 * t)

# Polynomial decay
beta_schedule = lambda t: 1.0 / (1 + 0.1 * t)

# Constant (for exploration)
beta_schedule = lambda t: 0.5
```

## Test Cases

### Test 1: Rosenbrock Function
```python
def rosenbrock(x):
    """Rosenbrock function in d dimensions"""
    return np.sum(100 * (x[1:] - x[:-1]**2)**2 + (1 - x[:-1])**2)

# Setup
d = 10
theta_0 = np.random.randn(d)
L_0 = 0.1 * np.random.randn(d, 64)
beta_schedule = lambda t: 1.0 / (1 + 0.05 * t)

# Run
theta_final, history = adasmooth_zo_lr(
    theta_0, L_0, beta_schedule, K=64, T=500, oracle=rosenbrock
)

# Expected: theta_final should be close to [1, 1, ..., 1]
```

### Test 2: Quadratic Function
```python
def quadratic(x):
    """Simple quadratic: f(x) = ||x - x*||^2"""
    x_star = np.ones(len(x))
    return np.sum((x - x_star)**2)

# Should converge quickly
```

### Test 3: High-Dimensional (d=1000)
```python
# Test scalability
d = 1000
theta_0 = np.random.randn(d)
L_0 = 0.05 * np.random.randn(d, 128)

# Memory: O(Kd) = O(128 * 1000) â‰ˆ 1MB (float64)
# Much better than full covariance O(d^2) â‰ˆ 8GB!
```

## Expected Behavior

1. **Convergence**: Function values should decrease over iterations
2. **Adaptive exploration**: Early iterations explore broadly (large L), later iterations refine locally (small L)
3. **Weight distribution**: Successful samples get higher weights
4. **Scalability**: Should handle d=10^3 to 10^4 easily with K=64-128

## Deliverables

Please implement:
1. Core function `adasmooth_zo_lr()`
2. Helper functions for temperature schedules
3. Test suite with Rosenbrock and quadratic functions
4. Visualization of:
   - Convergence curve (best f-value vs iteration)
   - Weight distribution evolution
   - Covariance matrix norm ||L_t L_t^T|| over time
5. Comparison with standard ZO (fixed smoothing)

## Performance Metrics

Track and report:
- Final objective value
- Number of function evaluations
- Convergence rate (iterations to reach threshold)
- Memory usage (should be O(Kd))
- Time per iteration

---

**Note**: This algorithm is mathematically rigorous and proven to converge to stationary points at O(1/âˆšT) rate while adaptively managing the bias-variance tradeoff through learned smoothing.

ä½ è¯´å¾—å¯¹ï¼æˆ‘ä»”ç»†çœ‹äº†ä»£ç ï¼Œ`ZerothOrderOptimizer` çš„ SGD æ¨¡å¼ç¡®å®æ˜¯ç›´æ¥æ›´æ–°ï¼š

```python
if self.update_rule == 'sgd':
    for group in self.param_groups:
        for param in group['params']:
            if param.grad is None:
                continue
            lr = group['lr']
            param.add_(-lr * param.grad)  # ç›´æ¥æ›´æ–°ï¼Œä¸ç»´æŠ¤åŠ¨é‡
```

æ‰€ä»¥ **AdaSmooth-ZO å®Œå…¨å¯ä»¥ç»§æ‰¿ `ZerothOrderOptimizer`**ï¼

---

## ç»§æ‰¿æ–¹æ¡ˆå®ç°

```python
class AdaSmoothZO(ZerothOrderOptimizer):
    """
    AdaSmooth-ZO: Zeroth-Order Optimization with Adaptive Sampling
    
    Learns both the search mean (Î¸) and low-rank covariance (LL^T) through
    KL-regularized policy optimization with moment matching.
    
    Key differences from base class:
    - Uses low-rank sampling: x = Î¸ + LÂ·u (u ~ N(0, I_K))
    - Updates Î¸ via weighted averaging (not gradient descent)
    - Maintains adaptive covariance matrix L âˆˆ R^(dÃ—K)
    """
    
    def __init__(
        self,
        params: Iterator[torch.Tensor],
        lr: float = 1.0,  # Not used, kept for interface compatibility
        betas: Tuple[float, float] = (0.9, 0.99),  # Not used
        epsilon: float = 1e-8,  # Not used
        num_queries: int = 64,  # This becomes K (batch size)
        mu: float = 0.1,  # This becomes initial_sigma
        update_rule: str = 'sgd',  # Force SGD mode
        beta_init: float = 1.0,
        beta_decay: float = 0.05,
        beta_schedule: str = 'polynomial',
    ):
        """
        Args:
            params: Iterator of parameters to optimize
            num_queries: Batch size K (rank of covariance)
            mu: Initial smoothing scale for L_0
            beta_init: Initial temperature Î²_0
            beta_decay: Decay rate for temperature
            beta_schedule: 'constant', 'exponential', or 'polynomial'
        """
        # Force SGD update rule (we handle updates in estimate_gradient)
        super().__init__(
            params, lr, betas, epsilon, num_queries, mu, 
            update_rule='sgd'
        )
        
        self.K = num_queries  # Rename for clarity
        self.initial_sigma = mu
        self.beta_init = beta_init
        self.beta_decay = beta_decay
        self.beta_schedule_type = beta_schedule
        self.iteration = 0
        
        # Initialize low-rank smoothing matrix L for each parameter
        self._initialize_L()
        
        # History tracking
        self.history = {
            'f_values': [],
            'weights': [],
            'beta': [],
            'L_norms': []
        }
    
    def _initialize_L(self):
        """Initialize low-rank smoothing matrix L_0 âˆˆ R^(dÃ—K) for each param"""
        for group in self.param_groups:
            for param in group['params']:
                state = self.state[param]
                # L âˆˆ R^(dÃ—K), initialized as small Gaussian noise
                d = param.numel()
                state['L'] = self.initial_sigma * torch.randn(
                    d, self.K,
                    device=param.device,
                    dtype=param.dtype
                )
    
    def _get_beta(self) -> float:
        """Get current temperature Î²_t"""
        t = self.iteration
        
        if self.beta_schedule_type == 'constant':
            return self.beta_init
        elif self.beta_schedule_type == 'exponential':
            return self.beta_init * np.exp(-self.beta_decay * t)
        elif self.beta_schedule_type == 'polynomial':
            return self.beta_init / (1 + self.beta_decay * t)
        else:
            raise ValueError(f"Unknown schedule: {self.beta_schedule_type}")
    
    def estimate_gradient(self, closure):
        """
        AdaSmooth-ZO gradient estimation with adaptive low-rank covariance.
        
        NOTE: This doesn't actually compute gradients in the traditional sense.
        Instead, it:
        1. Samples from low-rank Gaussian: x = Î¸ + LÂ·u
        2. Computes exponential weights: w âˆ exp(-f(x)/Î²)
        3. Updates Î¸ via weighted averaging
        4. Updates L via weighted residuals
        5. Sets param.grad to mimic SGD update: grad = (Î¸_old - Î¸_new) / lr
        
        The base class SGD step will then do: Î¸ â† Î¸ - lrÂ·grad = Î¸_new âœ“
        """
        beta_t = self._get_beta()
        
        # We'll process each parameter separately (or flatten if needed)
        # For simplicity, let's assume single parameter tensor
        param = None
        for group in self.param_groups:
            for p in group['params']:
                param = p
                break
            if param is not None:
                break
        
        if param is None:
            raise ValueError("No parameters to optimize")
        
        state = self.state[param]
        L_t = state['L']  # Shape: (d, K)
        d, K = L_t.shape
        
        # Flatten current parameter
        theta_t = param.data.view(-1)  # Shape: (d,)
        
        # ===== 1. Low-Rank Sampling =====
        X = []  # Candidate solutions
        Y = []  # Function values
        U = []  # Latent directions
        
        for k in range(K):
            # Sample from latent space: u ~ N(0, I_K)
            u = torch.randn(K, device=param.device, dtype=param.dtype)
            U.append(u)
            
            # Map to parameter space: x = Î¸ + LÂ·u
            x_k = theta_t + L_t @ u
            
            # Set parameter and evaluate
            param.data = x_k.view_as(param)
            f_val = closure()
            
            if isinstance(f_val, torch.Tensor):
                f_val = f_val.item()
            
            X.append(x_k)
            Y.append(f_val)
        
        # Restore original shape temporarily
        param.data = theta_t.view_as(param)
        
        X = torch.stack(X)  # Shape: (K, d)
        Y = torch.tensor(Y, device=param.device, dtype=param.dtype)
        U = torch.stack(U)  # Shape: (K, K)
        
        # ===== 2. Compute Weights (KL-divergence) =====
        # Exponential weighting: v_k = exp(-y_k / Î²)
        V = torch.exp(-Y / beta_t)
        
        # Normalize: w_k = v_k / Î£v_j
        W = V / V.sum()
        
        # ===== 3. Update Mean (via weighted averaging) =====
        # Î¸_{t+1} = Î£ w_k Â· x_k
        theta_new = torch.sum(W.unsqueeze(1) * X, dim=0)  # Shape: (d,)
        
        # ===== 4. Update Smoothing Matrix (Low-Rank) =====
        # Compute residuals: x_k - Î¸_{t+1}
        residuals = X - theta_new.unsqueeze(0)  # Shape: (K, d)
        
        # Weighted residuals: âˆšw_k Â· (x_k - Î¸_{t+1})
        weighted_residuals = torch.sqrt(W).unsqueeze(1) * residuals  # (K, d)
        
        # Stack as columns: L_{t+1} = [c_1, c_2, ..., c_K]
        L_new = weighted_residuals.T  # Shape: (d, K)
        
        # Update L in state
        state['L'] = L_new
        
        # ===== 5. Trick: Set gradient to achieve Î¸_new via SGD =====
        # We want: Î¸ â† Î¸ - lrÂ·grad = Î¸_new
        # So: grad = (Î¸ - Î¸_new) / lr
        lr = self.param_groups[0]['lr']
        pseudo_grad = (theta_t - theta_new) / lr
        
        # Set gradient
        param.grad = pseudo_grad.view_as(param)
        
        # ===== 6. Track History =====
        self.history['f_values'].append(Y.cpu().numpy())
        self.history['weights'].append(W.cpu().numpy())
        self.history['beta'].append(beta_t)
        self.history['L_norms'].append(torch.norm(L_new).item())
        
        self.iteration += 1
        
        # Return weighted mean loss (as a tensor for consistency)
        weighted_loss = torch.sum(W * Y)
        return weighted_loss


class AdaSmoothZO_MultiParam(ZerothOrderOptimizer):
    """
    AdaSmooth-ZO for models with multiple parameter tensors.
    Flattens all parameters into a single vector for unified covariance.
    """
    
    def __init__(
        self,
        params: Iterator[torch.Tensor],
        lr: float = 1.0,
        betas: Tuple[float, float] = (0.9, 0.99),
        epsilon: float = 1e-8,
        num_queries: int = 64,
        mu: float = 0.1,
        update_rule: str = 'sgd',
        beta_init: float = 1.0,
        beta_decay: float = 0.05,
        beta_schedule: str = 'polynomial',
    ):
        super().__init__(
            params, lr, betas, epsilon, num_queries, mu, 
            update_rule='sgd'
        )
        
        self.K = num_queries
        self.initial_sigma = mu
        self.beta_init = beta_init
        self.beta_decay = beta_decay
        self.beta_schedule_type = beta_schedule
        self.iteration = 0
        
        # Store parameter shapes for flattening/unflattening
        self.param_shapes = []
        self.param_numels = []
        total_numel = 0
        
        for group in self.param_groups:
            for param in group['params']:
                self.param_shapes.append(param.shape)
                numel = param.numel()
                self.param_numels.append(numel)
                total_numel += numel
        
        self.total_dim = total_numel
        
        # Single unified L matrix for all parameters
        # Store in first parameter's state for convenience
        first_param = None
        for group in self.param_groups:
            for param in group['params']:
                first_param = param
                break
            if first_param is not None:
                break
        
        self.state[first_param]['L'] = self.initial_sigma * torch.randn(
            self.total_dim, self.K,
            device=first_param.device,
            dtype=first_param.dtype
        )
        self.state[first_param]['is_storage'] = True
        
        self.history = {
            'f_values': [],
            'weights': [],
            'beta': [],
            'L_norms': []
        }
    
    def _get_beta(self) -> float:
        """Get current temperature Î²_t"""
        t = self.iteration
        
        if self.beta_schedule_type == 'constant':
            return self.beta_init
        elif self.beta_schedule_type == 'exponential':
            return self.beta_init * np.exp(-self.beta_decay * t)
        elif self.beta_schedule_type == 'polynomial':
            return self.beta_init / (1 + self.beta_decay * t)
        else:
            return self.beta_init / (1 + self.beta_decay * t)
    
    def _flatten_params(self) -> torch.Tensor:
        """Flatten all parameters into a single vector"""
        flat = []
        for group in self.param_groups:
            for param in group['params']:
                flat.append(param.data.view(-1))
        return torch.cat(flat)
    
    def _unflatten_to_params(self, flat: torch.Tensor):
        """Unflatten vector back to parameters"""
        offset = 0
        for group in self.param_groups:
            for param, shape, numel in zip(
                group['params'], self.param_shapes, self.param_numels
            ):
                param.data = flat[offset:offset+numel].view(shape)
                offset += numel
    
    def _get_L(self) -> torch.Tensor:
        """Get the unified L matrix"""
        for group in self.param_groups:
            for param in group['params']:
                if 'is_storage' in self.state[param]:
                    return self.state[param]['L']
        raise ValueError("L matrix not found")
    
    def _set_L(self, L_new: torch.Tensor):
        """Set the unified L matrix"""
        for group in self.param_groups:
            for param in group['params']:
                if 'is_storage' in self.state[param]:
                    self.state[param]['L'] = L_new
                    return
    
    def estimate_gradient(self, closure):
        """AdaSmooth-ZO gradient estimation for multi-parameter models"""
        beta_t = self._get_beta()
        
        # Get current flattened parameters and L
        theta_t = self._flatten_params()
        L_t = self._get_L()
        d, K = L_t.shape
        
        # ===== 1. Low-Rank Sampling =====
        X = []
        Y = []
        
        for k in range(K):
            u = torch.randn(K, device=theta_t.device, dtype=theta_t.dtype)
            x_k = theta_t + L_t @ u
            
            # Unflatten and evaluate
            self._unflatten_to_params(x_k)
            f_val = closure()
            
            if isinstance(f_val, torch.Tensor):
                f_val = f_val.item()
            
            X.append(x_k)
            Y.append(f_val)
        
        X = torch.stack(X)
        Y = torch.tensor(Y, device=theta_t.device, dtype=theta_t.dtype)
        
        # ===== 2. Compute Weights =====
        V = torch.exp(-Y / beta_t)
        W = V / V.sum()
        
        # ===== 3. Update Mean =====
        theta_new = torch.sum(W.unsqueeze(1) * X, dim=0)
        
        # ===== 4. Update Smoothing Matrix =====
        residuals = X - theta_new.unsqueeze(0)
        weighted_residuals = torch.sqrt(W).unsqueeze(1) * residuals
        L_new = weighted_residuals.T
        
        self._set_L(L_new)
        
        # ===== 5. Set pseudo-gradients =====
        lr = self.param_groups[0]['lr']
        flat_grad = (theta_t - theta_new) / lr
        
        offset = 0
        for group in self.param_groups:
            for param, numel in zip(group['params'], self.param_numels):
                param.grad = flat_grad[offset:offset+numel].view_as(param)
                offset += numel
        
        # ===== 6. Track History =====
        self.history['f_values'].append(Y.cpu().numpy())
        self.history['weights'].append(W.cpu().numpy())
        self.history['beta'].append(beta_t)
        self.history['L_norms'].append(torch.norm(L_new).item())
        
        self.iteration += 1
        
        weighted_loss = torch.sum(W * Y)
        return weighted_loss
```

---

## ä½¿ç”¨æ–¹æ³•

```python
# å•å‚æ•°æ¨¡å‹
optimizer = AdaSmoothZO(
    [model_param],  # å•ä¸ªå‚æ•°å¼ é‡
    lr=1.0,  # å¿…é¡»è®¾ä¸º1.0ï¼ˆå› ä¸ºæˆ‘ä»¬ç›´æ¥æ›´æ–°ï¼‰
    num_queries=64,  # K
    mu=0.1,  # initial_sigma
    beta_init=1.0,
    beta_decay=0.05,
    beta_schedule='polynomial'
)

# å¤šå‚æ•°æ¨¡å‹
optimizer = AdaSmoothZO_MultiParam(
    model.parameters(),
    lr=1.0,
    num_queries=64,
    mu=0.1,
    beta_init=1.0,
    beta_decay=0.05
)

# è®­ç»ƒ
def closure():
    return loss_fn(model(x), y)

for i in range(iterations):
    loss = optimizer.step(closure)
```

---

## å…³é”®æŠ€å·§

1. **ä¼ªæ¢¯åº¦è®¾ç½®**ï¼š`grad = (Î¸_old - Î¸_new) / lr`ï¼Œä½¿å¾— SGD æ­¥éª¤ `Î¸ â† Î¸ - lrÂ·grad = Î¸_new`
2. **å¼ºåˆ¶ lr=1.0**ï¼šç¡®ä¿ `Î¸ - grad = Î¸_new`
3. **å¼ºåˆ¶ update_rule='sgd'**ï¼šé¿å… Adam åŠ¨é‡å¹²æ‰°
4. **ä½ç§© L å­˜å‚¨åœ¨ state**ï¼šæ¯ä¸ªå‚æ•°ç»´æŠ¤è‡ªå·±çš„ Lï¼ˆå•å‚æ•°ç‰ˆï¼‰æˆ–ç»Ÿä¸€å­˜å‚¨ï¼ˆå¤šå‚æ•°ç‰ˆï¼‰

è¿™æ ·å°±å®Œç¾èå…¥ä½ çš„ç°æœ‰æ¡†æ¶äº†ï¼ğŸ‰