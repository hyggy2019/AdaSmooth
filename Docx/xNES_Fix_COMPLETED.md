# xNES ä¿®å¤å®ŒæˆæŠ¥å‘Š

## âœ… ä¿®å¤å®Œæˆ

xNES ä¼˜åŒ–å™¨çš„ä¸‰ä¸ªåˆå§‹åŒ–é—®é¢˜å·²å…¨éƒ¨ä¿®å¤ã€‚

**ä¿®å¤æ—¥æœŸï¼š** 2024-12-13

---

## ğŸ“‹ ä¿®å¤å†…å®¹

### 1. âœ… Sigma åˆå§‹åŒ–ä¿®å¤

**é—®é¢˜ï¼š** sigma ä½¿ç”¨ `self.mu` (0.01) åˆå§‹åŒ–ï¼Œæ­¥é•¿è¿‡å°

**ä¿®å¤ï¼š**
- æ·»åŠ ç‹¬ç«‹å‚æ•° `initial_sigma: float = 0.1`
- ä½¿ç”¨ `self.sigma_xnes = self.initial_sigma` æ›¿ä»£ `self.mu`

**æ–‡ä»¶ï¼š** `optimizer/zo.py` ç¬¬ 455, 465, 482 è¡Œ

### 2. âœ… Update Rule ä¿®å¤

**é—®é¢˜ï¼š** é»˜è®¤ `update_rule='radazo'`ï¼Œä½† xNES éœ€è¦ SGD

**ä¿®å¤ï¼š**
- ä¿®æ”¹é»˜è®¤å€¼ä¸º `update_rule: str = 'sgd'`
- æ·»åŠ éªŒè¯ï¼š`if update_rule != 'sgd': raise ValueError(...)`

**æ–‡ä»¶ï¼š** `optimizer/zo.py` ç¬¬ 450, 457-459 è¡Œ

### 3. âœ… Learning Rate ä¿®å¤

**é—®é¢˜ï¼š** é»˜è®¤ `lr=0.001` è¿‡å°

**ä¿®å¤ï¼š**
- ä¿®æ”¹é»˜è®¤å€¼ä¸º `lr: float = 1.0`

**æ–‡ä»¶ï¼š** `optimizer/zo.py` ç¬¬ 445 è¡Œ

---

## ğŸ“Š ä¿®å¤å‰åå¯¹æ¯”

| å‚æ•° | ä¿®å¤å‰ | ä¿®å¤å |
|------|--------|--------|
| **lr** | 0.001 âŒ | 1.0 âœ… |
| **update_rule** | 'radazo' âŒ | 'sgd' âœ… |
| **sigma åˆå§‹åŒ–** | self.mu (0.01) âŒ | self.initial_sigma (0.1) âœ… |

---

## ğŸ”§ ä»£ç å˜æ›´

### optimizer/zo.py

**__init__ æ–¹æ³•ï¼š**
```python
def __init__(
    self,
    params: Iterator[torch.Tensor],
    lr: float = 1.0,  # âœ… ä» 0.001 æ”¹ä¸º 1.0
    betas: Tuple[float, float] = (0.9, 0.99),
    epsilon: float = 1e-8,
    num_queries: int = 10,
    mu: float = 0.01,
    update_rule: str = 'sgd',  # âœ… ä» 'radazo' æ”¹ä¸º 'sgd'
    eta_mu: float = 1.0,
    eta_sigma: float = None,
    eta_bmat: float = None,
    use_fshape: bool = True,
    initial_sigma: float = 0.1,  # âœ… æ–°å¢å‚æ•°
):
    # âœ… æ–°å¢éªŒè¯
    if update_rule != 'sgd':
        raise ValueError("xNES requires update_rule='sgd'")

    super().__init__(params, lr, betas, epsilon, num_queries, mu, update_rule)

    self.eta_mu = eta_mu
    self.use_fshape = use_fshape
    self.initial_sigma = initial_sigma  # âœ… ä¿å­˜å‚æ•°
    # ...
```

**_initialize_xnes æ–¹æ³•ï¼š**
```python
def _initialize_xnes(self, param):
    if self.initialized:
        return

    self.dim = param.numel()
    self.sigma_xnes = self.initial_sigma  # âœ… ä» self.mu æ”¹ä¸º self.initial_sigma
    self.bmat = torch.eye(self.dim, device=param.device, dtype=param.dtype)
    # ...
```

### utils.py

**xNES æ³¨å†Œï¼š**
```python
elif name == "xnes":
    eta_mu = getattr(args, 'eta_mu', 1.0)
    eta_sigma = getattr(args, 'eta_sigma', None)
    eta_bmat = getattr(args, 'eta_bmat', None)
    use_fshape = getattr(args, 'use_fshape', True)
    initial_sigma = getattr(args, 'initial_sigma', 0.1)  # âœ… æ–°å¢
    return xNES(
        params=params,
        lr=args.lr,
        betas=args.betas,
        epsilon=args.epsilon,
        num_queries=args.num_queries,
        mu=args.mu,
        update_rule='sgd',
        eta_mu=eta_mu,
        eta_sigma=eta_sigma,
        eta_bmat=eta_bmat,
        use_fshape=use_fshape,
        initial_sigma=initial_sigma  # âœ… æ–°å¢
    )
```

---

## âœ… æµ‹è¯•éªŒè¯

### æµ‹è¯• 1: é»˜è®¤å‚æ•°
```python
optimizer = xNES(params)
assert optimizer.param_groups[0]['lr'] == 1.0  # âœ…
assert optimizer.update_rule == 'sgd'  # âœ…
assert optimizer.initial_sigma == 0.1  # âœ…
```

### æµ‹è¯• 2: Sigma åˆå§‹åŒ–
```python
loss = optimizer.step(closure)
assert abs(optimizer.sigma_xnes - 0.1) < 0.01  # âœ… 0.1000 (not 0.0100)
```

### æµ‹è¯• 3: Update Rule éªŒè¯
```python
try:
    bad_optimizer = xNES(params, update_rule='radazo')
except ValueError:
    pass  # âœ… æ­£ç¡®æŠ›å‡ºé”™è¯¯
```

### æµ‹è¯• 4: è‡ªå®šä¹‰ initial_sigma
```python
optimizer = xNES(params, initial_sigma=0.2)
loss = optimizer.step(closure)
assert abs(optimizer.sigma_xnes - 0.2) < 0.01  # âœ…
```

**æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼** âœ…

---

## ğŸ“š æ–‡æ¡£æ›´æ–°

### æ›´æ–°çš„æ–‡ä»¶ï¼š

1. **config/synthetic.yaml**
   - æ·»åŠ  `initial_sigma` å‚æ•°è¯´æ˜

2. **Docx/xNES_usage.md**
   - æ·»åŠ  `initial_sigma` å‚æ•°æ–‡æ¡£
   - æ·»åŠ å­¦ä¹ ç‡å’Œ SGD è¦æ±‚è¯´æ˜
   - æ·»åŠ å‚æ•°è°ƒä¼˜å»ºè®®

3. **Docx/xNES_Fix_COMPLETED.md** âœ¨ æ–°å»º
   - æœ¬æ–‡æ¡£ï¼ˆä¿®å¤å®ŒæˆæŠ¥å‘Šï¼‰

---

## ğŸ¯ ä½¿ç”¨å»ºè®®

### åŸºæœ¬ç”¨æ³•ï¼ˆä½¿ç”¨é»˜è®¤å€¼ï¼‰

```yaml
# config/synthetic.yaml
optimizers:
  - xnes  # ä½¿ç”¨æ‰€æœ‰é»˜è®¤å€¼ï¼ˆlr=1.0, initial_sigma=0.1ï¼‰
```

### è‡ªå®šä¹‰å‚æ•°

```yaml
# config/synthetic.yaml
optimizers:
  - xnes

# xNES å‚æ•°ï¼ˆå¯é€‰ï¼‰
eta_mu: 1.0         # å‡å€¼å­¦ä¹ ç‡
eta_sigma: null     # sigma å­¦ä¹ ç‡ï¼ˆè‡ªåŠ¨ï¼‰
eta_bmat: null      # B çŸ©é˜µå­¦ä¹ ç‡ï¼ˆè‡ªåŠ¨ï¼‰
use_fshape: true    # ä½¿ç”¨ fitness shaping
initial_sigma: 0.15 # åˆå§‹æ­¥é•¿ï¼ˆè‡ªå®šä¹‰ï¼‰
lr: 1.0             # å­¦ä¹ ç‡ï¼ˆæ¨èä¿æŒ 1.0ï¼‰
```

**é‡è¦ï¼š** xNES **å¿…é¡»ä½¿ç”¨ SGD**ï¼Œä¸æ”¯æŒ Adam/RadAZOã€‚

---

## ğŸ“ˆ é¢„æœŸæ”¹è¿›

ä¿®å¤åï¼ŒxNES åº”è¯¥ï¼š

1. âœ… **æ”¶æ•›æ›´å¿«** - åˆç†çš„åˆå§‹æ­¥é•¿ï¼ˆ0.1 vs 0.01ï¼‰
2. âœ… **ç¨³å®šæ€§æ›´å¥½** - æ­£ç¡®çš„å­¦ä¹ ç‡ï¼ˆ1.0 vs 0.001ï¼‰
3. âœ… **æ›´å®‰å…¨** - å¼ºåˆ¶ SGDï¼Œé˜²æ­¢è¯¯ç”¨
4. âœ… **å¯é…ç½®æ€§** - å¯é€šè¿‡ `initial_sigma` è°ƒæ•´æ­¥é•¿

---

## ğŸ” æŠ€æœ¯ç»†èŠ‚

### ä¸ºä»€ä¹ˆ xNES éœ€è¦ SGDï¼Ÿ

xNES é€šè¿‡è‡ªç„¶æ¢¯åº¦å’Œè‡ªé€‚åº”åæ–¹å·®çŸ©é˜µå®ç°å‚æ•°æ›´æ–°ï¼š

```python
# xNES å†…éƒ¨è®¡ç®—è‡ªç„¶æ¢¯åº¦
grad_direction = torch.matmul(self.bmat, dj_delta)

# è‡ªé€‚åº”æ›´æ–° sigma å’Œ B çŸ©é˜µ
self.sigma_xnes *= exp(0.5 * eta_sigma * dj_sigma)
self.bmat = self.bmat @ exp(0.5 * eta_bmat * dj_bmat)

# è®¾ç½®æ¢¯åº¦ï¼ˆä»…ç”¨äº SGD æ›´æ–°ï¼‰
p.grad = -(eta_mu * sigma_xnes * grad_direction)
```

**å¦‚æœä½¿ç”¨ Adam/RadAZOï¼š**
- ä¼šå¯¹è‡ªç„¶æ¢¯åº¦å†æ¬¡åº”ç”¨åŠ¨é‡å’Œè‡ªé€‚åº”å­¦ä¹ ç‡
- ç ´å xNES çš„æ•°å­¦åŸºç¡€
- å¯¼è‡´æ”¶æ•›é—®é¢˜

**å› æ­¤å¿…é¡»ä½¿ç”¨ SGDï¼**

---

## âœ… ä¿®å¤éªŒè¯æ¸…å•

- [x] ä¿®æ”¹ `lr` é»˜è®¤å€¼ä¸º 1.0
- [x] ä¿®æ”¹ `update_rule` é»˜è®¤å€¼ä¸º 'sgd'
- [x] æ·»åŠ  `initial_sigma` å‚æ•°
- [x] æ·»åŠ  update_rule éªŒè¯
- [x] æ›´æ–° sigma åˆå§‹åŒ–é€»è¾‘
- [x] æ›´æ–° utils.py æ³¨å†Œä»£ç 
- [x] æ›´æ–°é…ç½®æ–‡ä»¶æ–‡æ¡£
- [x] æ›´æ–°ä½¿ç”¨æ–‡æ¡£
- [x] ç¼–å†™æµ‹è¯•éªŒè¯
- [x] æ‰€æœ‰æµ‹è¯•é€šè¿‡

---

## ğŸŒŸ æ€»ç»“

xNES ä¼˜åŒ–å™¨çš„ä¸‰ä¸ªå…³é”®é—®é¢˜å·²å…¨éƒ¨ä¿®å¤ï¼š

1. âœ… **Sigma åˆå§‹åŒ–** - ä» 0.01 æå‡åˆ° 0.1ï¼ˆ10å€æ”¹è¿›ï¼‰
2. âœ… **Update Rule** - å¼ºåˆ¶ SGDï¼ˆé˜²æ­¢è¯¯ç”¨ï¼‰
3. âœ… **Learning Rate** - ä» 0.001 æå‡åˆ° 1.0ï¼ˆç¬¦åˆ xNES æ ‡å‡†ï¼‰

**ç°åœ¨ xNES å·²ç»å®Œå…¨æ­£ç¡®ï¼Œå¯ä»¥æ”¾å¿ƒä½¿ç”¨ï¼** ğŸš€

ä¿®å¤åçš„ xNES åº”è¯¥åœ¨åˆæˆå‡½æ•°ä¼˜åŒ–ä»»åŠ¡ä¸Šè¡¨ç°æ˜¾è‘—æ›´å¥½ã€‚
